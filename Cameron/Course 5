Recurrent neural networks - process sequential data, use directed cycles to form memory

Update hidden state to capture previous inputs in sequence
Hidden State - stored information about previous inputs to influence future predictions 

Back propagation through time -  Adjusts network weights by computing gradients with respect to weights over the sequence

RNN is unrolled - network where each layer is the state of RNN at a specific time step
Forward Pass - input passed through unrolled network, computes hidden states and out put at each state

Vanilla RNN - single hidden state passed through time steps

⁃ Many-to-many architecture: input and output sequences have the same length, example is name entity recognition
 ⁃ Many-to-one architecture: input is a sequence, output is a single number, example is sentiment classification
⁃ One-to-many architecture: input is a single number, output is a sequence, example is music generation

Language model estimates the probability of a particular sequence of words

Generates sequences by iteratively predicting the next word 

Gated Recurrent Unit - adds gating mechanism to control the flow of info within the network

Gates decide what to do with information at each time step (retain, update or discard)

LSTM - also uses gates to control flow 

Forget gate - decides what to discard
Input gate- decides what new info to add to cell state
Output gate- decides what to output 

Word embeddings - vector representations of words 
Similar words have similar numerical values 

Word2vec - predicts word relationships 
-predicts current word based on surrounding context
-predicts surrounding context using current word 

Negative sampling - Reduce computational cost by selecting negative samples or words that do not appear in context of the sentence. 
This helps teach the model which words shouldn’t be predicted. 


Embedding matrix - matrix where each row is a vector representation of a word 
In the matrix these similar words will have similar embedding vectors

GloVe - Global vectors for Word representation
Focuses on how often words appear together in the same context
Embeddings closer in vector space are closer in meaning

 Beam Search - Selects the top k most probable sequences at each time step to explore more options.
Ends at given maximum length or at end token
Selects the sequence with the highest total probability.
Avoids getting stuck at local optima which can happen in greedy search


Beam width - k value - how many top sequences we will keep track of

Attention Model - Assigns different weights to parts of the input sequence at each time step, to reflect importance for generating the output


